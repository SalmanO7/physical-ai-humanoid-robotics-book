---
title: Data Model
---

# Data Model: Physical AI & Humanoid Robotics

## Key Entities

### 1. Humanoid Robot
Represents the physical (simulated) entity capable of locomotion, perception, and manipulation.

- **Attributes**:
  - `joints`: A collection of actuated joints, each with properties like position, velocity, and effort limits.
  - `sensors`: An array of integrated sensors (LiDAR, Depth Camera, IMU), each providing specific environmental data.
  - `actuators`: Mechanisms responsible for physical movement and manipulation.
  - `kinematic_model`: A representation (e.g., URDF) defining the robot's links, joints, and their spatial relationships.
  - `state`: Current operational status (e.g., idle, navigating, manipulating, error).

### 2. Environment
Represents the simulated world where the robot operates, containing static and dynamic obstacles, and target objects.

- **Attributes**:
  - `physics_properties`: Parameters governing physical interactions (e.g., gravity, friction, collision detection) within the simulation.
  - `visual_assets`: 3D models and textures defining the appearance of the environment and objects.
  - `spatial_maps`: Representations of the environment's geometry (e.g., occupancy grids, point clouds) used for navigation and perception.
  - `objects`: A collection of `Object` entities present in the environment.

### 3. Navigation Command
Represents pre-defined navigation commands used to direct the robot.

- **Attributes**:
  - `command_text`: The text of the navigation command (e.g., "navigate to kitchen", "move to target").
  - `semantic_meaning`: The interpreted high-level intent and parameters extracted from the command (e.g., "navigate to location", "move to target").
  - `associated_actions`: A link to the `Action Plan` generated from this command.

### 4. Object
Represents items within the environment that the robot can perceive and manipulate.

- **Attributes**:
  - `physical_properties`: Characteristics like mass, dimensions, material, and collision geometry.
  - `visual_features`: Descriptors used for identification (e.g., color, shape, texture).
  - `semantic_label`: A human-readable identifier (e.g., "red cube", "blue sphere").
  - `pose`: Position and orientation (e.g., x, y, z, roll, pitch, yaw) within the environment.
  - `state`: Current status (e.g., grasped, at rest, in motion).

### 5. Action Plan
Represents a sequence of high-level tasks generated by the LLM to achieve a given `Navigation Command`.

- **Attributes**:
  - `task_list`: An ordered list of discrete high-level tasks (e.g., `NAVIGATE_TO(location)`, `IDENTIFY_OBJECT(object_id)`, `MANIPULATE_OBJECT(object_id, action)`).
  - `dependencies`: Inter-task dependencies (e.g., object identification must precede manipulation).
  - `execution_status`: Current state of the plan (e.g., pending, in_progress, completed, failed).
  - `generated_by`: Reference to the LLM responsible for generation.

### 6. ROS 2 System
Represents the middleware facilitating communication and control within the robot's software stack.

- **Attributes**:
  - `nodes`: Individual executable processes within ROS 2.
  - `topics`: Named buses over which nodes exchange messages.
  - `services`: Request/reply communication mechanisms.
  - `messages`: Data structures for inter-node communication.
  - `actions`: Long-running, preemptable tasks (e.g., navigation goals, manipulation sequences).
  - `interfaces`: Definitions of messages, services, and actions (e.g., `.msg`, `.srv`, `.action` files).